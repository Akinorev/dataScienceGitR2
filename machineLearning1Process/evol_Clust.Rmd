---
title: "Cluster"
author: "Master Data Science: Verónica Gómez, Carlos Grande, Pablo Olmos"
date: "2020"
output:
  html_document:
    theme: united
    code_folding: "hide"
    toc: yes
    toc_float: yes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(cluster)
library(dplyr)
library(ggplot2)
library(readr)
library(factoextra)
library(Rtsne)
library(caret)
library(lattice)
library(glmnet)
library(pscl)
library(ROCR)
library(VarReg)
library(mgcv)
library(brew)
library(bsplus)
library(DMwR2)
library(car)
library(carData)
library(caret)
library(cluster)
library(dplyr)
library(egg)
library(expss)
library(factoextra)
library(gclus)
library(GGally)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(Hmisc)
library(htmltools)
library(kableExtra)
library(knitr)
library(magrittr)
library(mlbench)
library(RColorBrewer)
library(readr)
library(Rtsne)
library(tidyr)
library(tidyverse)
library(VIM)
library(scales)
library(plyr)
library(PerformanceAnalytics)
library(corrplot)
library(leaps)
library(plotly)
library(mboost)
library(nnet)
library(stargazer)


```
# Cluster Análisis

  Dada la complejidad y dispersión de nuestra base de Datos de casas, desde el departamento comercial nos solicitan una primera segmentación de la cartera para facilitar la comprensión y operatividad de la misma. Para ello desde el departamento de Analítica planteamos realizar dentro de las técnicas no supervisadas un Análisis Cluster en 3 grandes grupos que nos permite segmentar de forma óptima alrededor de la variable precio.


### Distancia de Gower

Nuestra base de datos al estar compuesta por variables continuas, oridinales y categóricas decidimos emplear al   Distancia de Gower para el posterior Clustering.
Es una medida numérica de cuan separados están los items de nuestra base de Datos.


### Algoritmo PAM

Una vez calculada la matriz de distancia emplearemos el algoritmo PAM, basado en una partición de medoids. Es un método muy similar a k-means, pero es mucho más robusto a la presencia de Outliers como es en nuestro caso. Es un procedimiento de agrupación iterativa.


### Selección del Número de Clúster y respresentación en un PCA

A fín de manejar de forma viable la segmentación seleccionamos 3 clusters, que sin ser el número óptimo facilitado por el algoritmo es el número propuesto desde el departamento comercial para optimizar adecuadamente sus recurcos.
Una vez seleccionado el número de Clusters vamos a representarlos en un plano mediante una técnica de reducción de la dimensionalidad Lineal el PCA.
Obteniendo la siguiente representación.

```{r reduccion del codigo}
df_mas <- read.csv ("kc_house_data.csv")
gower_dist <- daisy(df_mas, metric = "gower")

sil_width <- c(NA)
for(i in 2:3){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}

set.seed=737
k <- 3
pam_fit <- pam(gower_dist, diss = TRUE, k)
pam_results <- df_mas %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))

data<- subset(df_mas, select=c("price", "bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront",                                        "view","condition","grade","sqft_above","sqft_basement","sqft_living15","sqft_lot15"))
pca<- prcomp(data, scale=TRUE)

pam_fit$clustering <- as.character(pam_fit$clustering)
pca_data <- data.frame(pca$x, cluster=pam_fit$clustering)

ggplot(pca_data, aes(x=PC1, y=PC2, color=cluster)) + geom_point()

```


Vemos que esta representacón no es satisfactoria ya que existe mucha superposición en los clustering y visualmente no percibimos grandes diferencias.
Por ello vamos a plantear una alternativa al PCA pero para datos no lineales, el TSNE.



### TSNE

Algoritmo de reducción de dimensionalidad no lineal, encuentra patrones en los datos mediante la identificación de grupos observados basados en la similitud de puntos de datos con múltiples características.
Asigna los datos multidimensionales creados en la Distancia de Gower anteriormente calculada a un espacio dimensional menor.
Nos va a permitir usando la Distancia de Gower la representación de nuestros clusters creados por el Algoritmo PAM


```{r tsne}
set.seed=737
tsne_obj <- Rtsne(gower_dist,perplexity = 80, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

Y vemos efectivamente  en la reprfesentación de TSNE que existen relaciones No lineales entre nuestros datos y con esta técnica conseguimos posicionar e identificar de una manera mucho más concreta y detallada un mayor número de clusters.
En nuestra representación de los 3 clusters del PAM vemos se han agrupado diferentes casas que vemos con la representación del TSNE podrían ser considerados como nuevos grupos por lo que vamos plantear un nuevo análisis jeráquico en función de los ejes de TSNE

### Hierarchical clustering

Agrupaciones de Clusters anidados de forma Aglomerativa dentro del plano XY del TSNE.
Obtenemos la siguiente representación:

```{r Cluster Jeraquico}

set.seed(767)
cluster_hierarchical=hclust(dist(tsne_obj$Y), method = "ward.D")
df_join<-cbind(tsne_data,df_mas)
df_join$hclust = factor(cutree(cluster_hierarchical, 9))
prueba <- subset( df_join, select = -cluster )
ggplot(aes(x = X, y = Y), data = prueba) +
  geom_point(aes(color = hclust))

```


A partir de estos Clusters jerárquicos vamos a reagrupar en 3 grandes Clusters en función de la variable Precio obteniendo:

```{r Cluster final}

#setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/Z_PRACTICA_MACHINE_LEARNING/machineLearning1Process")
df_cluster <- read.csv ("cluster.csv")

#descriptivo en cada Clúster
set.seed(767)
df_cluster %>%
        group_by(hclust) %>%
        summarise(num_casas=n(), precio_medio= mean(price), room=mean(bedrooms),baths=mean(bathrooms) ,
                  tamanyo=mean(sqft_living), anyo=floor(mean(yr_built)) ,grade=mean(grade), condicion=mean(condition),        visitas=mean(view))



df_cluster$hclust=as.numeric(df_cluster$hclust)
df_cluster$Cluster_final[df_cluster$hclust==2 | df_cluster$hclust==8] <- "top"
df_cluster$Cluster_final[df_cluster$hclust==1 | df_cluster$hclust==5 | df_cluster$hclust==9] <- "low"
df_cluster$Cluster_final[df_cluster$hclust==3 | df_cluster$hclust==4 | df_cluster$hclust==6 | df_cluster$hclust==7  ] <- "med"



df_cluster %>%
        group_by(Cluster_final) %>%
        summarise(num_casas=n(), precio_medio= mean(price), room=mean(bedrooms),baths=mean(bathrooms) ,
                  tamanyo=mean(sqft_living), anyo=floor(mean(yr_built)) ,grade=mean(grade), condicion=mean(condition),visitas=mean(view))

ggplot(df_cluster, aes(x=price, fill=Cluster_final, color=Cluster_final)) +
  geom_histogram(aes(y=..density..), binwidth=3)+  geom_density(aes(color=Cluster_final)) 

```







