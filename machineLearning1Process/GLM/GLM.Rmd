---
title: "MachineLearning GLM"
  author: "Carlos Grande Nuñez, Veronica Gomez Gomez y Pablo Olmos Martinez"
date: "13/02/2020"
output:
  html_document:
    code_folding: hide
    theme: united
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(dplyr)
library(ggplot2)
library(readr)
library(factoextra)
library(Rtsne)
library(caret)
library(lattice)
library(glmnet)
library(pscl)
library(ROCR)
library(VarReg)
library(mgcv)
library(brew)
library(bsplus)
library(DMwR2)
library(car)
library(carData)
library(caret)
library(cluster)
library(dplyr)
library(egg)
library(expss)
library(factoextra)
library(ggthemes)
library(gclus)
library(GGally)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(Hmisc)
library(htmltools)
library(kableExtra)
library(knitr)
library(magrittr)
library(mlbench)
library(RColorBrewer)
library(readr)
library(Rtsne)
library(tidyr)
library(tidyverse)
library(VIM)
library(scales)
library(plyr)
library(corrplot)
library(leaps)
library(plotly)
library(mboost)
library(nnet)
library(stargazer)
library(readxl)
library(dplyr)
library(ggplot2)
library(GGally)
library(Hmisc)
library(corrplot)
library(PerformanceAnalytics)
library(gains)
library(pROC)
library(rpart)
library(modelplotr)
library(plotly)

```




# GLM 
 La principal aportación de la regresión logística consiste en extener el modelo teórico de la regresión
simple y multivariante, para aplicarla a problemas en los que la variable de salida del modelo es discreta
o categórica, en lugar de continua.

### Objetivo de Negocio y Definición del Target
Nuestro objetivo de negocio es generar estructural departamentales y para ello necesitamos definir dos grandes grupos principalmente.
Por un Lado buscamos los perfiles de casas de Alto Poder adquisitivo versus el Resto.
Para ello nos apoyaremos de la Regresión Logística y necesitaremos definir un cluster 1/0 en el que las casas con valor 1 son aquellas 
de alto poder adquisitivo o Precio alto y 0 el resto de casas.
¿cómo definimos este punto de corte?
Vamos a leer nuestra BBDD y estudiar la variable precio para encontrar el punto de corte óptimo para nuestro análisis


### Lectura de Nuestra Base cluster.

```{r TRAIN CONTROL }

#setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/Z_PRACTICA_MACHINE_LEARNING/machineLearning1Process")
df_cluster <- read.csv ("../cluster.csv")
#setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/Z_PRACTICA_MACHINE_LEARNING/machineLearning1Process")
df_root <- read.csv ("../kc_house_data.csv")


df_cluster$hclust=as.numeric(df_cluster$hclust)
df_cluster$cluster_final[df_cluster$hclust==2 | df_cluster$hclust==8] <- "top"
df_cluster$cluster_final[df_cluster$hclust==1 | df_cluster$hclust==5 | df_cluster$hclust==9] <- "low"
df_cluster$cluster_final[df_cluster$hclust==3 | df_cluster$hclust==4 | df_cluster$hclust==6 | df_cluster$hclust==7  ] <- "med"


```


## Density Price Plot  de la variable Precio 

Antes de plantear un punto de corte para crear una variable dicomtómica y estudiar un GLM  plantearemos una Árbol de regression para estudiar el comportamiento de la variable Precio que tiene una distribucón de este estilo, no sigue una distribución normal. Parece una Gamma.

```{r distribución de la variable Precio en nuestro DataSet}

df_cluster$hclust<-as.factor(df_cluster$hclust)
ggplot(data=df_cluster, aes(x=price, group=hclust ,fill=hclust)) +
    geom_density(adjust=1.5)

df_cluster$cluster<-as.factor(df_cluster$cluster)
ggplot(data=df_cluster, aes(x=price, group=cluster ,fill=cluster)) +
    geom_density(adjust=1.5)

densidad <- density(df_cluster$price)
plot(densidad, main="Gráfica de densidad de la variable precio de la vivienda" , xlim=c(0,4000000))
polygon(densidad, col="red")



```

## Variable cluster Binaria. 30% de Las casas más caras 1 resto 0

```{r Definicion y Creacion de la Variable cluster}
describe(df_cluster$price)

histograma <- ggplot(df_cluster, aes(x=price)) +
  ggtitle("Precio de las viviendas") +
  theme_fivethirtyeight() +
  geom_histogram(color="#28324a", fill="#3c78d8")
histograma


```


Vemos que la distribución de la variable precio es muy asintótica hacia la derecha, es decir hay muchos valores extremos que 
puede que nos desvirtuen el análisis de la media, El punto de corte que vamos a determinar es el percentil 75 de la variable precio.
La variable cluster será 1 cuando los precios sean mayores a percentil 75 de la variable precio y 0 en caso contrario

#### Contrucción de la variable Target 
Vemos en el gráfico Box Plot que la distribución de la población cluster 1, las viviendas caras, tiene muchos valores extremos. 
Para realziar un buen análisis deberáimos extraerlos pero es importante para nuestro negocio por lo que vamos a mantenerlos 
a ver si somos capaces de realizar una buena predicción.

```{r Contrucción del cluster precio vivienda}

target1 <- filter(df_cluster, price > 645000)
summary(target1)

target0 <- filter(df_cluster, price <= 645000)
summary(target0)

df_cluster$cluster=as.numeric(df_cluster$hclust)


df_cluster$target[df_cluster$price> 645000] <- '1'
df_cluster$target[df_cluster$price<= 645000] <- '0'

table(df_cluster$target)


df_cluster$cluster<-as.factor(df_cluster$target)
ggplot(data=df_cluster, aes(x=price, group=target ,fill=target)) + 
  ggtitle("Precio de las viviendas por cluster") +
    geom_density(adjust=1.5)

 df_cluster %>%
  ggplot( aes(x=target, y=price, fill=target)) + 
    ggtitle("Precio de las viviendas por target") +
    geom_violin() +
    xlab("class") +
    theme(legend.position="none") +
    xlab("")



```
## EDA por cluster
Vamos a realizar un análisis descriptivo de las variables en función del cluster


#### Descripcion de las variables

- id: valor único (Primary key).
- date: fecha de venta de la vivienda.
- price: precio de venta. Variable seleccionada para la aplicación del modelo y su posterior predicción.
- bedrooms: número de habitaciones por vivienda.
- bathrooms: número de baños por vivienda.
- sqft_living: superficie de la vivienda en pies cuadrados (superficie escriturada).
- sqft_lot: superficie de la parcela de la vivienda en pies cuadrados (superficie parcelaria).
- floors: número de plantas por vivienda.
- waterfront: si la vivienda tiene vistas al mar.
- view: el número de veces que se ha visitado la vivienda desde su puesta en venta.
- condition*: el estado de la vivienda establecido mediante una variable numérica del 1 al 5.
- grade*: nota general de la vivienda propuesta por el sistema de puntuación de la zona del 1 al 13.
- sqft_above: superficie de la huella perimetral de la vivienda sobre rasante en pies cuadrados.
- sqft_basement: superficie de la vivienda bajo rasante en piés cuadrados
- yr_built: año de construcción de la vivienda
- yr_renovated: año de la renovación de la vivienda. En caso de no haber sido renovada este parámetro se ha igualado a 0.
- zipcode: codigo postal de la vivienda.
- lat: latitud de la coordenada de la vivienda medida en pies.
- long: longitud de la coordenada de la vivienda medida en pies.
- sqft_living15: superficie de la vivienda en el año 2015 (admite renovaciones).
- sqft_lot15: superficie de la parcela en el año 2015 (admite modificaciones)



```{r funciones auxiliares, include=FALSE, warning=FALSE}

phist <- function(df, bns = 50, varname) {
  p = ggplot(df, aes(x = df[[varname]])) + 
    geom_histogram(aes(y =..density..), 
                   colour = "#464159", 
                   fill = "#8bbabb", na.rm = TRUE) + 
    ggtitle("Diagrama Boxplot") + 
    ylab("Densidad") + xlab(varname) + 
    scale_x_continuous(labels = scales::comma) +
  stat_function(fun = dnorm, args = list(mean = mean(df[[varname]]), sd = sd(df[[varname]])))
return(p)
}
pbox <- function(col, varname){
p = qplot('1', col, geom="boxplot") +
  geom_boxplot(fill='#8bbabb', color="#464159") +
  ggtitle("Diagrama Boxplot") + ylab("Valores") + xlab(varname)
return(p)
}
p_barras <-function(df = df_cualitativas, dx, xlab){
p = ggplot(df, aes(unlist(dx), fill=unlist(dx))) +
  geom_bar(position="dodge", fill='#8bbabb', color="#464159") + 
  labs(x= xlab, y = 'Frecuencia', fill=NULL)
return(p)
}
p_barras2 <-function(df = df_cualitativas, dx, dy, xlab){
p = ggplot(df, aes(unlist(dx), fill=unlist(dy))) +
  geom_bar(position="dodge") + 
  labs(x= xlab, y = 'Frecuencia', fill=NULL) +
  scale_fill_manual(values=c("#8bbabb", "#6c7b95"))
return(p)
}

```


# Análisis Exploratorio de las Variables



```{r Filtrado cluster 1}
df_target_1 = filter(df_cluster, target == "1")
#Muestra de las primeras 5 filas de la base de datos
kable(head(df_target_1)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = TRUE)
  
#Tabla resumen con los principales estadísticos
kable(summary(df_target_1)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = TRUE)
```



```{r Filtrado cluster 0}
df_target_0 = filter(df_cluster, target == "0")
#Muestra de las primeras 5 filas de la base de datos
kable(head(df_target_0)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = TRUE)
  
#Tabla resumen con los principales estadísticos
kable(summary(df_target_0)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = TRUE)
```



**Estudio de la variable "price" (precio de venta).**
```{r estudio price, include=TRUE, warning=FALSE}
my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = price))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$price))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure


```

**Estudio de las variables "sqft_living"  y "sqft_living15" (Superficie de la vivienda).**
variable "sqft_living": superficie de la vivienda en pies cuadrados (superficie escriturada).

```{r estudio sqft_living, include=TRUE, warning=FALSE}
my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = sqft_living))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$sqft_living))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```



Variable "sqft_lot15":  superficie de la parcela en el año 2015 
```{r estudio sqft_living15 V2, include=TRUE, warning=FALSE}

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = sqft_lot15))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$sqft_lot15))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```
**Estudio de la variable "sqft_above" (superficie de la huella de la vivienda).**

```{r estudio sqft_above, include=TRUE, warning=FALSE}

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = sqft_above))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$sqft_above))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```
**Estudio de la variable "sqft_basement" (superficie bajo rasante).**


```{r estudio sqft_basement, include=TRUE, warning=FALSE}

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = sqft_basement))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$sqft_basement))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```
**Estudio de la variable "yr_built" (año de construcción de la vivienda).**

```{r estudio yr_built, include=TRUE, warning=FALSE}

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = yr_built))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$yr_built))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```

**Estudio de la variable "yr_renovated" (año de renovación de la vivienda).**


```{r estudio yr_renovated, include=TRUE, warning=FALSE}

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
df_cluster$target <- as.factor(df_cluster$target)
pBoxPlot <- ggplot(df_cluster, aes(x = target, y = yr_renovated))
bxp <- pBoxPlot + geom_boxplot(aes(color = target)) +
  scale_color_manual(values = my3cols)
geomHist <- ggplot(data=df_cluster, aes(df_cluster$yr_renovated))
gh <- geomHist + geom_histogram(aes(color = target)) +
      scale_color_manual(values = my3cols)
figure <- ggarrange(bxp, gh, 
                    labels = c("Boxplot", "Histogram"),
                    ncol = 2, nrow = 1)
figure
```

**Estudio de la variable "bathrooms" (Número de baños/aseos por vivienda):**


```{r var_bathrooms, include=TRUE, warning=FALSE}
var_bathrooms_1 = df_target_1$bathrooms
var_bathrooms_0 = df_target_0$bathrooms

name_1 = "target_1 bathrooms"
name_2 = "target_0 bathrooms"

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
# Tablas de frecuencias en función al mes y al año
summary(var_bathrooms_1)
summary(var_bathrooms_0)

pb1<-ggplot(df_target_1, aes(unlist(var_bathrooms_1), fill=unlist(var_bathrooms_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_bathrooms_0), fill=unlist(var_bathrooms_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)


figure <- ggarrange(pb1, pb2)
figure

out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(bathrooms)) %>% .$bathrooms,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(bathrooms)) %>% .$bathrooms),
                   probability = TRUE, main = "Comparativa Variable bathrooms")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)

```




**Estudio de la variable "bedrooms" (Número de habitaciones por vivienda):**

```{r bedrooms, include=TRUE, warning=FALSE, message=FALSE}

var_bedrooms_1 = df_target_1$bedrooms
var_bedrooms_0 = df_target_0$bedrooms

name_1 = "target_1 bedrooms"
name_2 = "target_0 bedrooms"

my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_bedrooms_1)
summary(var_bedrooms_0)
pb1<-ggplot(df_target_1, aes(unlist(var_bedrooms_1), fill=unlist(var_bedrooms_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_bedrooms_0), fill=unlist(var_bedrooms_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)


figure <- ggarrange(pb1, pb2)
figure

out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(bedrooms)) %>% .$bedrooms,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(bedrooms)) %>% .$bedrooms),
                   probability = TRUE, main = "Comparativa Variable Bedrooms")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)

```




**Estudio de la variable "floors" (Número de plantas por vivienda):**

```{r var_floors, include=TRUE, warning=FALSE}
var_floors_1 = df_target_1$floors
var_floors_0 = df_target_0$floors

name_1 = "target_1 floors"
name_2 = "target_0 floors"

my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_floors_1)
summary(var_floors_0)

pb1<-ggplot(df_target_1, aes(unlist(var_floors_1), fill=unlist(var_floors_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_floors_0), fill=unlist(var_floors_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)


figure <- ggarrange(pb1, pb2)
figure

out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(floors)) %>% .$floors,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(floors)) %>% .$floors),
                   probability = TRUE, main = "Comparativa Variable floors")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)

```


**Estudio de la variable "condition" (estado de la vivienda del 1 al 5):**

```{r var_condition, include=TRUE, warning=FALSE}
var_condition_1 = df_target_1$condition
var_condition_0 = df_target_0$condition

name_1 = "target_1 condition"
name_2 = "target_1 condition"

my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_condition_1)
summary(var_condition_0)


pb1<-ggplot(df_target_1, aes(unlist(var_condition_1), fill=unlist(var_condition_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_condition_0), fill=unlist(var_condition_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)


figure <- ggarrange(pb1, pb2)
figure

out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(condition)) %>% .$condition,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(condition)) %>% .$condition),
                   probability = TRUE, main = "Comparativa Variable condition")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)


```


**Estudio de la variable "waterfront" (viviendas frente a grandes masas de agua):**


```{r var_waterfront, include=TRUE, warning=FALSE}
var_waterfront_1 = df_target_1$waterfront
var_waterfront_0 = df_target_0$waterfront

name_1 = "target_1 waterfront"
name_2 = "target_0 waterfront"

my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_waterfront_1)
summary(var_waterfront_0)


pb1<-ggplot(df_target_1, aes(unlist(var_waterfront_1), fill=unlist(var_waterfront_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_waterfront_0), fill=unlist(var_waterfront_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)
figure <- ggarrange(pb1, pb2)
figure


```


**Estudio de la variable "grade" (nota general de la vivienda del 1 al 13):**

```{r var_grade, include=TRUE, warning=FALSE}
var_grade_1 = df_target_1$grade
var_grade_0 = df_target_0$grade

name_1 = "target_1 grade"
name_2 = "target_0 grade"

my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_grade_1)
summary(var_grade_0)


pb1<-ggplot(df_target_1, aes(unlist(var_grade_1), fill=unlist(var_grade_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_grade_0), fill=unlist(var_grade_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)

figure <- ggarrange(pb1, pb2)
figure


out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(grade)) %>% .$grade,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(grade)) %>% .$grade),
                   probability = TRUE, main = "Comparativa Variable grade")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)
```


**Estudio de la variable "view" (número de visitas que ha recibido la vivienda):**

```{r var_view, include=TRUE, warning=FALSE}
var_view_1 = df_target_1$view
var_view_0 = df_target_0$view

name_1 = "target_1 view"
name_2 = "target_0 view"
my3cols <- c("#E7B800", "#2E9FDF")
# Tablas de frecuencias en función al mes y al año
summary(var_view_1)
summary(var_view_0)

pb1<-ggplot(df_target_1, aes(unlist(var_view_1), fill=unlist(var_view_1))) +
  geom_bar(position="dodge", fill='blue', color="blue") + 
  labs(x= name_1, y = 'Frecuencia', fill=NULL)

pb2<-ggplot(df_target_0, aes(unlist(var_view_0), fill=unlist(var_view_0))) +
  geom_bar(position="dodge", fill='red', color="red") + 
  labs(x= name_2, y = 'Frecuencia', fill=NULL)




figure <- ggarrange(pb1, pb2)
figure


out = histbackback(list(Target_1 = df_cluster %>% filter(target == "1") %>% filter(!is.na(view)) %>% .$view,
                        Target_0 = df_cluster %>% filter(target == "0") %>% filter(!is.na(view)) %>% .$view),
                   probability = TRUE, main = "Comparativa Variable view")
# Colorear mitad izquierda y derecha del gráfico
barplot(-out$left, col="blue" , horiz=TRUE, space=0, add=TRUE, axes=FALSE)
barplot(out$right, col="red", horiz=TRUE, space=0, add=TRUE, axes=FALSE)
```

```{r}
summary (df_cluster)
```







# Correlaciones 
En el siguiente gráfico descriptivo vemos que apraecen altas y medias correlaciones entre algunas variables por lo que vamos a tener que realizar un trabajo con las mismas con Ridge y Lasso.

```{r Correlaciones por Grupo, include=TRUE, warning=FALSE}


pm <- ggpairs(
  df_cluster[, c( 7,8,9,10,11,17,18,24,25,30)],
  ggplot2::aes(colour=target))
pm

```


# Creación Base Train y Control


Definimos una semilla aleatoria y realizamos una partición en Train 70% y Test 30%.

```{r TRAIN CONTROL para el glm }

set.seed(737)
#inTraining <- createDataPartition(df_cluster$id, p = .7, list = FALSE)
#train    <- df_cluster[inTraining,]
#control  <- df_cluster[-inTraining,]

split_data <- function(data, trn = .7, val = .2, tst = .1) {
  set.seed(737)
  spec = c(train = trn, validate = val, test = tst)
  # cutter
  g = sample(cut(seq(nrow(data)), nrow(data)*cumsum(c(0,spec)), labels = names(spec)))
  # spliter
  data <- split(data, g)
  return(data)
}

data <- split_data(df_cluster, 0.7, 0.2, 0.1)
train <- data$train
control <- data$test
validation<- data$validate


table(train$target)
table(control$target)
table(validation$target)

```




# Ridge regression

  Intenta minimizar el RSS, ridge regression incorpora un término llamado shrinkage penalty que fuerza a que los coeficientes de los predictores tiendan a cero controlada por el parámetro λ. 
  Cuando λ=0 la penalización es nula y los resultados son equivalentes a los obtenidos por mínimos cuadrados, cuando λ=∞ todos los coeficientes son cero.
  La principal ventaja es la reducción de Varianza.
  Si  todos los predictores incluidos tienen coeficientes diferentes a cero (todos contribuyen al modelo) y aproximadamente de la misma magnitud, ridge regression tiende a funcionar mejor.
  
  Para realizar ridge regression se va a emplear la función glmnet() del paquete glmnet.
 
 
```{r tunning parameter LAMDA }

#matriz con las valores de los predictores para cada observación y un vector y=target variable respuesta
x <- model.matrix(target~  (bedrooms+ bathrooms+floors+sqft_living+grade+condition+view+waterfront+                           sqft_lot+sqft_above+sqft_basement+yr_built+yr_renovated+yr_renovated+
                            sqft_living15+sqft_lot15+Cluster_final), data = train)[, -1]
head(x)
y <- train$target
y <- as.integer(y)



# Para obtener un ajuste mediante ridge regression se indica argumento alpha=0.
modelos_ridge <- glmnet(x = x, y = y, alpha = 0)
plot(modelos_ridge, xvar = "lambda", label = TRUE)

```     
  
  Al aumentar el tamaño de los Lambda dismunuyen los coeficientes.
  Con el fin de identificar el valor de λ que da lugar al mejor modelo, se puede recurrir a Cross-Validation. La función cv.glmnet() calcula el cv-test-error, utilizando por defecto k=10.
  
 
```{r select  parameter LAMBDA in Ridge Regression}  
  
set.seed(737)
cv_error_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 15,
                            type.measure = "mse")
plot(cv_error_ridge)
```
  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -2, pero el menor error cuadrático medio se da para 17 variables regresoras y se mantiene constante. Es una de las grandes diferencias con Lasso.




```{r select  min parameter LAMBDA in Ridge Regression}  
  
# Valor lambda con el que se consigue el mínimo test-error
cv_error_ridge$lambda.min

```


```{r select  optimal  parameter LAMBDA in Ridge Regression}  
# Valor lambda óptimo: mayor valor de lambda con el que el test-error no se
# aleja más de 1 sd del mínimo test-error posible.
cv_error_ridge$lambda.1se
```

  
  
```{r modelo final Ridge}  
# Se muestra el valor de los coeficientes para el valor de lambda óptimo

modelo_final_ridge <- glmnet(x = x, y = y, alpha = 0, lambda = cv_error_ridge$lambda.1se)
coef(modelo_final_ridge)

```
  
  
# Lasso

  El método lasso, al igual que ridge regression, fuerza a que las estimaciones de los coeficientes de los predictores tiendan a cero. La diferencia es que lasso sí es capaz de fijar algunos de ellos exactamente a cero, lo que permite además de reducir la varianza, realizar selección de predictores.
  ∑i=1n(yi−β0−∑j=1pβjxij)2+λ∑j=1p|βj|=RSS+λ∑j=1p|βj|
  
  Cuando solo un pequeño número de predictores de entre todos los incluidos tienen coeficientes sustanciales y el resto tienen valores muy pequeños o iguales a cero, lasso genera mejores modelos.
  
  
Selección del tunning parameter λ
  
    Determinar el grado de penalización, seleccionamos un rango de valores de λ y se estima el cross-validation error resultante para cada uno, finalmente se selecciona el valor de λ para el que el error es menor y se ajusta de nuevo el modelo, esta vez empleando todas las observaciones.
    

```{r tunning parameter  }

modelos_lasso <- glmnet(x = x, y = y, alpha = 1)
plot(modelos_lasso, xvar = "lambda", label = TRUE)

set.seed(737)
cv_error_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 10)
plot(cv_error_lasso)

```     

  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -4, pero el menor error cuadrático medio se da para 3 variables regresoras. 


```{r tunning parameter Lasso LAMBDA }

cv_error_lasso$lambda.min
cv_error_lasso$lambda.1se

# Se reajusta el modelo con todas las observaciones empleando el valor de
# lambda óptimo

modelo_final_lasso <- glmnet(x = x, y = y, alpha = 1, lambda = cv_error_lasso$lambda.1se)
coef(modelo_final_lasso)
    
```    
  
  la ventaja del modelo final obtenido por lasso es que es mucho más simple ya que contiene únicamente 'n' predictores
  A continuación, ajustamos un modelo de regresión con el λ para las variables significativas 
  No obstante, como se observa en la gráfica del error podríamos obtener un modelo con sólo 10 variables cuyo error es muy similar. 
  Para ello buscamos el valor de λ para el cual obtenemos el primer conjunto con 10 variables.
  
 

```{r comparing Ridge and Lasso  }

par(mfrow = c(1,2))
plot(cv_error_ridge,ylab = "Mean Square Error ridge regression" )
abline(h = 120000)
plot(cv_error_lasso,ylab = "Mean Square Error lasso")
abline(h = 120000)

par(mfrow = c(1,1))
```   




# Regresión Logistica 

A partir de las variables del Lasso

```{r Regresión Logística 1}

train_glm1 = glm(target ~ floors + grade + condition + view + sqft_above + yr_built + sqft_living15 + Cluster_final , 
               family = binomial,
               data = train )
summary(train_glm1)

```
Viendo los resultados de la regresión logística podemos afirmar que todas las variables introducidas en el modelo son significativas a nivel estadístico, aunque podemos apreciar en las variables Floors y View un valor del estadísitico no demasiado alto lo cual nos dice que no son especialmete explicativas.

### Vamos a probar un modelo Nuevo Versión 2 excluyendo estas variables

```{r Regresión Logística 2}

train_glm2 = update(train_glm1, . ~ . - view - floors) # Eliminamos dos predictores
anova(train_glm1, train_glm2, test = "Chisq")

```

Esta salida nos dice que el ajuste es estadísticamente significativo pero a nivel negocio son importantes y en el primer modelo eran significativas por lo que las vamos a mantener.


## Intervalo de confianza de los parámetros

```{r Intervalo de confianza de los parámetros}
head(predict(train_glm1, type = "response")) # Probabilidades en escala de la salida
summary(train_glm1$fitted.values)

```



## Matriz de Confusión

### ROC Plot
Vemos en la Matriz de Confusión que el modelo tenemos problemas con Falsos Positivos, el modelo predice de forma notable una casa como de Precio bajo cuando son de precio Alto. Esto para nuestro negocio tiene implicaciones ya que lo que queremos evitar es no pronosticar bien las casas de Precio Alto.
 
```{r Matrix Confusion and Roc Plot}


predictions <- predict(train_glm1, train,type='response')
plot.roc(train$target, predictions)
table(pred = predictions > 0.5, obs = train$target)
data = as.numeric(predictions>0.5)
data=as.factor(data)
y_test=as.factor(train$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)


#VEro testing ROCit library
library(ROCit)
ROCit_obj <- rocit(score = predictions, class = train$target)
plot(ROCit_obj)
#END Vero testing
        

```


Tal y como se puede observar, sobre la curva ROC tenemos que tiende mas hacia los 90º que hacia los 45º. Esto lo que nos indica es que pese a tener falsos positivos el test ha salido bastante preciso.

Habria que mirar el AUC???

### Gain chart
```{r lift Chart Train and Test Population}


require(ROCR)

predictions <- predict(train_glm1, train,type='response')
pred<- prediction(predictions,  train$target)
gain <- performance(pred, "tpr", "rpp")
plot(gain, main = "Gain Chart Train Population")
abline(a=0,b=1)





```

Por la grafica obtenida se observa que al 70% aprox ya se tiene una respuesta del 98%. Esto permitira ahorrar unos costes de aproximadamente un 30% donde solo perderiamos el 2% de las respuestas.


### Lift chart


```{r Lift chart}
# lift chart
perf <- performance(pred,"lift","rpp")
plot(perf, main="lift curve")




```




## Diferencias entre la función Link Logit y Probit

¿Cual es azul y cual roja?

```{r differences Betwwen Logit and Probit}

# Diferencia entre el logit y el probit
X=seq(from=-4,to=4,by=0.1)
sigmoide=1/(1+exp(-X))
cumulative<-pnorm(X, 0, 1)
plot(sigmoide,type="l",col="red")
lines(cumulative,col="blue")


```

## Train Model Binomail Link Logit

```{r Train Model Binomail Link Logit }


model_logit1 = glm(target ~ floors + grade + condition + view + sqft_above + yr_built + sqft_living15 + Cluster_final , 
               family = binomial (link="logit") ,
               data = train )
summary(model_logit1)



```


### Evaluacion del modelo Logit


```{r  Evaluacion del Modelo Logit}


train$prediccion=predict(model_logit1,type="response")
Pred_auxiliar= prediction(train$prediccion, train$target, label.ordering = NULL)

CURVA_ROC_model_logit1_train <- performance(Pred_auxiliar,"tpr","fpr")
plot(CURVA_ROC_model_logit1_train)
abline(a=0,b=1)



## Capacidad del Modelo
#mean(as.numeric(train$target)-1)
#aggregate(train$prediccion~train$target,FUN=mean)


table(pred = train$prediccion > 0.25, obs = train$target)
data = as.numeric(train$prediccion>0.25)
data=as.factor(data)
y_test=as.factor(train$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)


```

### Obteniendo un threshold mas preciso para Logit
```{r obtaining threshold for Logit}
cutoffs <- seq(0.1,0.9,0.1)
accuracy <- NULL
for (i in seq(along = cutoffs)){
    prediction <- ifelse(model_logit1$fitted.values >= cutoffs[i], 1, 0) #Predicting for cut-off
accuracy <- c(accuracy,length(which(data$y ==prediction))/length(prediction)*100)
}

plot(cutoffs, accuracy, pch =19,type='b',col= "steelblue",
     main ="Logistic Regression", xlab="Cutoff Level", ylab = "Accuracy %")

```

### Validacion del modelo Logit con el dataset de Validation
```{r validation on test for Logit}


predictionsLogit <- predict(model_logit1, validation,type='response')
plot.roc(validation$target, predictionsLogit)
table(pred = predictionsLogit > 0.5, obs = validation$target)
data = as.numeric(predictionsLogit>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)
    # use caret and compute a confusion matrix
confusionMatrix(data, y_test)


```






## Train Model Binomail Link Probit

```{r Train Model Binomail Link Probit}

train$yr_built=as.numeric(train$yr_built)

model_probit1 = glm(target ~ floors + grade + condition + view + sqft_above + yr_built + sqft_living15 + Cluster_final , 
               family = binomial (link="probit") ,
               data = train )
summary(model_probit1)
```



### Evaluacion del modelo Probit


```{r  Evaluacion del Modelo Probit}


train$prediccion=predict(model_probit1,type="response")
Pred_auxiliar= prediction(train$prediccion, train$target, label.ordering = NULL)

CURVA_ROC_model_probit1_train <- performance(Pred_auxiliar,"tpr","fpr")
plot(CURVA_ROC_model_probit1_train)
abline(a=0,b=1)



## Capacidad del Modelo
#mean(as.numeric(train$target)-1)
#aggregate(train$prediccion~train$target,FUN=mean)


table(pred = train$prediccion > 0.25, obs = train$target)
data = as.numeric(train$prediccion>0.25)
data=as.factor(data)
y_test=as.factor(train$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)


```

### Validacion del modelo Probit con el dataset de Control
```{r validation on test Probit}


predictionsProbit <- predict(model_probit1, validation,type='response')
plot.roc(validation$target, predictionsProbit)
table(pred = predictionsProbit > 0.5, obs = validation$target)
data = as.numeric(predictionsProbit>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)
    # use caret and compute a confusion matrix
confusionMatrix(data, y_test)


```










