---
title: "Memoria Machine Learning I."
author: "Carlos Grande Nuñez, Veronica Gomez Gomez y Pablo Olmos Martinez"
date: "30/04/2020"
output:
  html_document:
    theme: united
    code_folding: "hide"
    toc: yes
    toc_float: yes
---

# Libraries and fucntions
## Libraries
```{r}
# basics
library(dplyr)
library(ggplot2)
library(tidyr)
library(tictoc)
#library(kableExtra)
# decision tree
library(gmodels)
```

## Functions
```{r}
split_data <- function(data, trn = .7, val = .2, tst = .1) {
  set.seed(737)
  spec = c(train = trn, validate = val, test = tst)
  # cutter
  g = sample(cut(seq(nrow(data)), nrow(data)*cumsum(c(0,spec)), labels = names(spec)))
  # spliter
  data <- split(data, g)
  return(data)
}
```



# 1 Loading Data
## 1.1 Reading the Data from the CSV
```{r}
file <- 'cluster.csv'
df_houses <-  read.csv(file)
head(df_houses)
```
## 1.2 Adding a binary target
```{r}
quantile <- df_houses$price %>% quantile(0.75)
df_houses$target <- ifelse(df_houses$price>quantile, 'a', 'b')
df_houses <- df_houses %>% select(6, 8:25, 29)
df_houses
table(df_houses$target)
head(df_houses)
```


## 1.3 Split data in train validation and test
```{r}
data <- split_data(df_houses)
train <- data$train
validation <- data$validate
test <- data$test


x_train <- train[, 1:(ncol(train)-1)]
y_train <- train[, (ncol(train))] %>% as.factor()
x_val <- validation[, 1:(ncol(train)-1)]
y_val <- validation[, (ncol(train))] %>% as.factor()
x_test <- test[, 1:(ncol(train)-1)]
y_test <- test[, (ncol(train))] %>% as.factor()

table(train$target)
table(validation$target)
table(test$target)
```

# 2 Random Forest over cluster
## 2.1 Training the model
```{r}
library(randomForest)

# The model
rf <- randomForest(x_train, y_train, ntree = 500)
rf
```
El error del 7.12% es conocido como 'out-of-bag' ratio de error. El cual se estima durante el ensamble del modelo. Al terminar la construcción del modelo, se estima el ratio de error del voto de cada árbol para cada una de las muestras de la tabla obteniendo este resultaado El problema con este tipo de error es que no tiene en cuenta el azar del modelo.

## 2.2 Estimating the kappa error
El Coeficiente kappa de Cohen, en cambio, es una medida estadística que ajusta el efecto del azar en la proporción de la concordancia observada para elementos cualitativos (variables categóricas). En general se cree que es una medida más robusta que el simple cálculo del porcentaje de concordancia, ya que κ tiene en cuenta el acuerdo que ocurre por azar.
```{r}
library(vcd)
library(grid)
kp <- kappa(rf$confusion)
output= sprintf("The Kappa error for the model is: %s", kp)
print(output)
```

## 2.3 Evaluating the model performance on validation
```{r}
pred <- predict(rf, validation)
table_train <- CrossTable(y_val, pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('Actual', 'Precicted'))
accuracy <- sum(diag(table_train$t))/sum(table_train$t)
output= sprintf("The accuracy of the RF on Validation is: %s", accuracy)
print(output)
```

# 3 Evaluating random forest in a simulated competition (Best parameters search)
Para conseguir una mayor precisión en el ajuste del modelo vamos a usar repetir 5 veces un k-fold cross validation de 10. Además añadiremos nuevas opciones a las muestra de train. En primer lugar vamos a activar las opciones de *savePredictions* y *classProbs*,  para guardar los residuos de las predicciones y sus probabilidades, para poder mostrar la curva de ROC posteriormente. También activaremos la opción de *summaryFunction* a *twoClassSummary* para poder calcular el área de la curva de ROC (AUC).

```{r}
library(caret)
library(lattice)
ctrl <- trainControl(method = 'repeatedcv',
                     number = 10, repeats = 4,
                     selectionFunction = 'best',
                     savePredictions = TRUE,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
grid_rf <- expand.grid(mtry = c(4, 8, 16, 18))
```


# 3.1 Running the grid search
Vamos a usar la función *train()* con el objeto *ctrl* y aplicaremos la métrica de 'ROC' para seleccionar el mejor modelo
```{r}
tic()
set.seed(737)
m_rf <- train(target ~ ., data = train, method = 'rf', metric = 'ROC', trControl = ctrl, tuneGrid = grid_rf)
m_rf
toc()
```

```{r}
m_rf_final <- m_rf$pred %>% filter(mtry == 8)
m_rf_final
```

## 3.2 Visualizing the performance by ROC curve.
```{r}
library(randomForest)

# The model
rf <- randomForest(x_train, y_train, ntree = 500, mtry = 8)
rf
```


```{r}
pred <- predict(rf, test)
table_train <- CrossTable(y_test, pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('Actual', 'Precicted'))
accuracy <- sum(diag(table_train$t))/sum(table_train$t)
output= sprintf("The test accuracy of the tree on test is: %s", accuracy)
print(output)
```





```{r}
library(pROC)

roc_rf <- roc(m_rf_final$obs, m_rf_final$a)
plot(roc_rf, colr = 'red', legacy.axes = TRUE)
```
```{r}
library(ROCit)

ROCit_obj <- rocit(score = m_rf_final$a, class = m_rf_final$obs)
plot(ROCit_obj)
```

