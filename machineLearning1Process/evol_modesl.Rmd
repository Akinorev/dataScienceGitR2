---
title: "Models"
author: "Master Data Science: Verónica Gómez, Carlos Grande, Pablo Olmos"
date: "2020"
output:
  html_document:
    theme: united
    code_folding: "hide"
    toc: yes
    toc_float: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(cluster)
library(dplyr)
library(ggplot2)
library(readr)
library(factoextra)
library(Rtsne)
library(caret)
library(lattice)
library(glmnet)
library(pscl)
library(ROCR)
library(VarReg)
library(mgcv)
library(brew)
library(bsplus)
library(DMwR2)
library(car)
library(carData)
library(caret)
library(cluster)
library(dplyr)
library(egg)
library(expss)
library(factoextra)
library(gclus)
library(GGally)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(Hmisc)
library(htmltools)
library(kableExtra)
library(knitr)
library(magrittr)
library(mlbench)
library(RColorBrewer)
library(readr)
library(Rtsne)
library(tidyr)
library(tidyverse)
library(VIM)
library(scales)
library(plyr)
library(PerformanceAnalytics)
library(corrplot)
library(leaps)
library(plotly)
library(mboost)
library(nnet)
library(stargazer)


```

## Modelo GAM. Variable Dependiente Precio Continua

Como pudimos comprobar en el trimestre anterior los tradicionales modelos lineales a menudo fallan cuando planteamos problemas no lineales que suele ser lo habitual en la vida real. Con esta técnica queremos identificar los efectos no lineales de nuestros dantos dentro de un modelo predictivo de regresión.



```{r GAM Price}

set.seed(737)
inTraining <- createDataPartition(df_cluster$id, p = .7, list = FALSE)
gamtrain    <- df_cluster[inTraining,]
gamcontrol  <- df_cluster[-inTraining,]

model_gam3 <- gam (sqrt(price)~ s(lat,long,k=20) +s(sqft_living,k=8)  +s(sqft_living15) + s(bathrooms,k=8)  + s(grade,k=3)   +(waterfront) +   s(view,k=3) + s(yr_built) , data=gamtrain, method="REML")

summary(model_gam3)


```

## Modelo GAM. Variable Dependiente Binary

En función del Target 1/0 con la función link logistica
Regularización Lasso

```{r GAM Binary}
train      <- read.csv ("train.csv")
validation <- read.csv ("validation.csv")
# Modelo
train$Cluster_final <- relevel(train$Cluster_final, ref = "med")
model_gam_log2 <- gam (target~ s(lat,long) +s(sqft_living) + s(grade) + s( sqft_above)  + s(yr_built) + s(sqft_living15) +floors+ s(view,k=3) + condition + Cluster_final,
                       data=train, family =  binomial("logit"), method="REML")

  

summary(model_gam_log2)
```

```{r matriz confuson GAM Binary}
predictions_val <- predict(model_gam_log2, validation,type='response')
data = as.numeric(predictions_val>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)
```


## Modelos GLM. Regresión Logistica (Logit)

En función del Target 1/0 con la función link logit
Regularización Lasso

```{r Regresión Logistica (Logit)}

train_glm1 = glm(target ~ floors + grade + condition + view + sqft_above + yr_built + sqft_living15 + Cluster_final , 
               family = binomial,
               data = train )
summary(train_glm1)
predictions <- predict(train_glm1, validation,type='response')
data = as.numeric(predictions>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)

```


## Modelos GLM. Regresión Logistica (Probit)

En función del Target 1/0 con la función link Probit
Regularización Lasso

```{r Regresión Logistica (Probit)}
train$yr_built=as.numeric(train$yr_built)

model_probit1 = glm(target ~ floors + grade + condition + view + sqft_above + yr_built + sqft_living15 + Cluster_final , 
               family = binomial (link="probit") ,
               data = train )
summary(model_probit1)
predictions <- predict(model_probit1, validation,type='response')
data = as.numeric(predictions>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)

```

## SVM Kernel PolyNomial

En función del Target 1/0 
Dummy variable Categorica Cluster Final
Cálculo de Hiperparámetros 
SVM Kernel Polynomial 

```{r Preprocessing SVM}
df_cluster_dummies <- dummy_cols(train,select_columns =c ("Cluster_final"))
df_cluster_dummies_validation <- dummy_cols(validation,select_columns =c ("Cluster_final"))


myvars <- c('bedrooms' ,'bathrooms' ,'sqft_living', 'sqft_lot' ,'floors' ,'waterfront' ,'view' ,'condition' ,'grade' ,'sqft_above' ,'sqft_basement','yr_built', 'yr_renovated','sqft_living15' ,'sqft_lot15' ,'target' ,'Cluster_final_med', 'Cluster_final_low', 'Cluster_final_top')

train_svm <- df_cluster_dummies[myvars]
validate_svm <- df_cluster_dummies_validation[myvars]

```




```{r SVM Polinomial }

#modelo_svm <- svm(target ~ ., data = train_svm, 
#                  type='C-classification',
#                  kernel = "polynomial", 
#                  cost = 3.3,
#                  gamma= 7.5,
#                  iter.max=1,
#                  scale = TRUE)

```





