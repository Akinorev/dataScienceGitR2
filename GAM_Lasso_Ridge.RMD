---
title: "GAM"
author: "Master Data Science: Verónica Gómez, Carlos Grande, Pablo Olmos"
date: "2020"
output:
  html_document:
    theme: united
    code_folding: "hide"
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(dplyr)
library(ggplot2)
library(readr)
library(factoextra)
library(Rtsne)
library(caret)
library(lattice)
library(glmnet)
library(pscl)
library(ROCR)
library(VarReg)
library(mgcv)
library(brew)
library(bsplus)
library(DMwR2)
library(car)
library(carData)
library(caret)
library(cluster)
library(dplyr)
library(egg)
library(expss)
library(factoextra)
library(faraway)
library(gclus)
library(GGally)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(Hmisc)
library(htmltools)
library(ISLR)
library(kableExtra)
library(knitr)
library(magrittr)
library(mice)
library(mlbench)
library(RColorBrewer)
library(readr)
library(Rtsne)
library(sos)
library(tidyr)
library(tidyverse)
library(VIM)
library(nortest)
library(scales)
library(plyr)
library(PerformanceAnalytics)
library(corrplot)
library(leaps)
library(plotly)
library(mboost)
library(nnet)
library(stargazer)

```

## Planteamiento del Problema

Intetaremos a través de las nuevas técnicas conocidase en el trimestre para intentar predecir la variable dependiente Precio de Compra.
Vamos a aboradar el proceso de dos maneras:
1.-Por un lado tenemos la Variable Precio de forma continúa, ésta será nuestra variable objetivo dentro de un modelo GAM
2.-y por otro depúes de haber terminado el análisis clúster de las poblaciones de viviendas dividido en Top, Med y Low intentaremos predecir la pertenencia a 
cada unos de los grupos a través de un GAM, link function multiclass logistic.

Por último destacar como punto clave que la literatura recomienda usar modelos GAM en el caso de que exista un problema de Geolocalización en los datos con
coordenadar X,Y como es nuestro caso. 

# Secuencia del Trabado Modelo GAM1:

Lectura del Fichero  
Ridge y Lasso selección de variables
GAM model
Validación e interpretación de los datos.

 

# 1. Modelo GAM. Variable Dependiente Precio Continua.

https://pdfs.semanticscholar.org/dea4/adaaf06e6fc99179a2620b7a031188c6e532.pdf

  Como pudimos comprobar en el trimestre anterior los tradicionales modelos lineales a menudo fallan cuando planteamos problemas no lineales que suele ser lo habitual en la vida real. Con esta técnica queremos identificar los efectos no lineales de nuestros dantos dentro de un modelo predictivo de regresión.

  Los GAM son un buen punto intermedio entre los modelos lineales (lm) y los modelos no paramétricos (p.e. Random Forest)
  
  Planteamos el uso del modelo GAM dentro de la Regression Lineal ya que hemos comprobado que nuestro modelo contiene efectos no lineales.

  1.- Las relaciones entre los predictores individuales y la variable dependiente siguen una función Smooth (suavizada), que puede ser lienal o no.
  2.- Podemos estimar estas relaciones simultáneamente y predecir g(e(Y)) simplemente sumándolas
  3.- Matemáticamente, GAM como
      
          g(E(Y))= a + s1(x1) + ....+ sp(zp)
          
        donde Y es la variable dependiente y g(Y) link function que vincula Y con x1,x2...xp
        
    s(x1), s(x2)...s(xp)  son las smooth non parametrics fuctions (small set of parameters)
    Notar quepodemos usar en GLM 
    for example Logit Link g(E(y))= log ( p(Y=1)/P(Y=0)) 
    
  Las razones por las que vamos a emplear GAM son:
  
  1.-Interpretabilidad
  2.-Fexibiliad y automatización
  3.-Regularización

  
## 1Interpretabilidad

  La interepretación de un impacto marginal de una variable NO depende del valor de otras variables en el modelo. Por tanto con un simple vistazo al output del modelo podemos hacer una interepretación de la variable en el modelo.
  La posibilidad de controlar el 'smoothness' de las funciones predictoras.
  
  
## 2Flexibility and Automation

  GAM es capaz de capturar patrones no lineales. Estos patrones van desde "hockey sticks" que se producen cuando se observa un "sharp" en la respuesta a varios tipos de "mountain shaped". En los fitting de modelos parametrics de regression estos efectos no lienales son habitualmente recogidos a través de polinomios el cual genera un modelo poco eficiente.
  Con el modelo GAM no tenemos estos problemas, las funciones de predcicción se derivan automaticamente durante la estimación del modelo. No necesitamos saber por adelantado qué tipo de funciones necesitaremos. 
  Ahorro de tiempo y nos encontará patrones perdidos en los modelos paramétricos.
  

## 3regularización

  Nos permite controlar el "smoothness" de la funciones predictoras para eviar el Overfiting. Controlando "the wiggliness" evitar el sesgo.
  Las funciones de penalización del GAM tienen conexiones con la regresión Bayessiana y la L2.


Smoothers
  
    Snoothers son las clave del GAM, tenemos tres clases de Smoothers:
    
    1.-Local Regression (loess)
    2.-Smoothing Splines
    3.-Regression Splines ( B-splines, P-splines, thin plate splines)
    
  RS, computacionalmente barato, es que emplearemos más habitualmente. La principal ventaja es que puede ser expresada como una combinación lienal de un conjunto finito de funciones básicas las cuales No dependen de la variable Y. S(x) =>  Bp,1(x)...Bp,k(x) funciones básicas donde B es la matriz dónde se albergan y B=[B1:B2:.....:Bp] son los coeficinetes. 
  El número de funciones básicas depende de el número de "inner knots"(m) (TODO), p numero de parámetros.
  
  Número de funciones básicas K = p + 1 + m



# Lectura Datos

Definimos una semilla aleatoria y realizamos una partición en Train 80% y Test 20%.

```{r TRAIN CONTROL }

setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/Z_PRACTICA_MACHINE_LEARNING/machineLearning1Process")
df_cluster <- read.csv ("cluster.csv")

#descriptivo en cada Clúster
set.seed(767)
df_cluster %>%
        group_by(hclust) %>%
        summarise(num_casas=n(), precio_medio= mean(price), room=mean(bedrooms),baths=mean(bathrooms) ,
                  tamanyo=mean(sqft_living), anyo=floor(mean(yr_built)) ,grade=mean(grade), condicion=mean(condition),        visitas=mean(view))


df_cluster$hclust=as.numeric(df_cluster$hclust)
df_cluster$Cluster_final[df_cluster$hclust==2 | df_cluster$hclust==8] <- "top"
df_cluster$Cluster_final[df_cluster$hclust==1 | df_cluster$hclust==5 | df_cluster$hclust==9] <- "low"
df_cluster$Cluster_final[df_cluster$hclust==3 | df_cluster$hclust==4 | df_cluster$hclust==6 | df_cluster$hclust==7  ] <- "med"



df_cluster %>%
        group_by(Cluster_final) %>%
        summarise(num_casas=n(), precio_medio= mean(price), room=mean(bedrooms),baths=mean(bathrooms) ,
                  tamanyo=mean(sqft_living), anyo=floor(mean(yr_built)) ,grade=mean(grade),condicion=mean(condition),visitas=mean(view))


set.seed(737)
inTraining <- createDataPartition(df_cluster$id, p = .8, list = FALSE)
train    <- df_cluster[inTraining,]
control  <- df_cluster[-inTraining,]


```


# Ridge regression

  Intenta minimizar el RSS, ridge regression incorpora un término llamado shrinkage penalty que fuerza a que los coeficientes de los predictores tiendan a cero controlada por el parámetro λ. 
  Cuando λ=0 la penalización es nula y los resultados son equivalentes a los obtenidos por mínimos cuadrados, cuando λ=∞ todos los coeficientes son cero.
  La principal ventaja es la reducción de Varianza.
  Si  todos los predictores incluidos tienen coeficientes diferentes a cero (todos contribuyen al modelo) y aproximadamente de la misma magnitud, ridge regression tiende a funcionar mejor.
  
  Para realizar ridge regression se va a emplear la función glmnet() del paquete glmnet.
 
 
```{r tunning parameter LAMDA }

#matriz con las valores de los predictores para cada observación y un vector y=target variable respuesta
x <- model.matrix(price~  (bedrooms+ bathrooms+floors+sqft_living+grade+condition+view+waterfront+                           sqft_lot+sqft_above+sqft_basement+yr_built+yr_renovated+yr_renovated+
                            sqft_living15+sqft_lot15), data = train)[, -1]
head(x)
y <- train$price


# Para obtener un ajuste mediante ridge regression se indica argumento alpha=0.
modelos_ridge <- glmnet(x = x, y = y, alpha = 0)
plot(modelos_ridge, xvar = "lambda", label = TRUE)

```     
  
  Al aumentar el tamaño de los Lambda dismunuyen los coeficientes.
  Con el fin de identificar el valor de λ que da lugar al mejor modelo, se puede recurrir a Cross-Validation. La función cv.glmnet() calcula el cv-test-error, utilizando por defecto k=10.
  
 
```{r select  parameter LAMBDA in Ridge Regression}  
  
set.seed(737)
cv_error_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 15,
                            type.measure = "mse")
plot(cv_error_ridge)
```
  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -2, pero el menor error cuadrático medio se da para 15 variables regresoras y se mantiene constante. Es una de las grandes diferencias con Lasso.




```{r select  min parameter LAMBDA in Ridge Regression}  
  
# Valor lambda con el que se consigue el mínimo test-error
cv_error_ridge$lambda.min

```


```{r select  optimal  parameter LAMBDA in Ridge Regression}  
# Valor lambda óptimo: mayor valor de lambda con el que el test-error no se
# aleja más de 1 sd del mínimo test-error posible.
cv_error_ridge$lambda.1se
```

  
  
```{r modelo final Ridge}  
# Se muestra el valor de los coeficientes para el valor de lambda óptimo

modelo_final_ridge <- glmnet(x = x, y = y, alpha = 0, lambda = cv_error_ridge$lambda.1se)
coef(modelo_final_ridge)

```
  
  
# Lasso

  El método lasso, al igual que ridge regression, fuerza a que las estimaciones de los coeficientes de los predictores tiendan a cero. La diferencia es que lasso sí es capaz de fijar algunos de ellos exactamente a cero, lo que permite además de reducir la varianza, realizar selección de predictores.
  ∑i=1n(yi−β0−∑j=1pβjxij)2+λ∑j=1p|βj|=RSS+λ∑j=1p|βj|
  
  Cuando solo un pequeño número de predictores de entre todos los incluidos tienen coeficientes sustanciales y el resto tienen valores muy pequeños o iguales a cero, lasso genera mejores modelos.
  
  
Selección del tunning parameter λ
  
    Determinar el grado de penalización, seleccionamos un rango de valores de λ y se estima el cross-validation error resultante para cada uno, finalmente se selecciona el valor de λ para el que el error es menor y se ajusta de nuevo el modelo, esta vez empleando todas las observaciones.
    

```{r tunning parameter  }

modelos_lasso <- glmnet(x = x, y = y, alpha = 1)
plot(modelos_lasso, xvar = "lambda", label = TRUE)

set.seed(737)
cv_error_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 10)
plot(cv_error_lasso)

```     

  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -4, pero el menor error cuadrático medio se da para 3 variables regresoras. 


```{r tunning parameter Lasso LAMBDA }

cv_error_lasso$lambda.min
cv_error_lasso$lambda.1se

# Se reajusta el modelo con todas las observaciones empleando el valor de
# lambda óptimo

modelo_final_lasso <- glmnet(x = x, y = y, alpha = 1, lambda = cv_error_lasso$lambda.1se)
coef(modelo_final_lasso)
    
```    
  
  la ventaja del modelo final obtenido por lasso es que es mucho más simple ya que contiene únicamente 'n' predictores
  A continuación, ajustamos un modelo de regresión con el λ para las variables significativas 
  No obstante, como se observa en la gráfica del error podríamos obtener un modelo con sólo 7 variables cuyo error es muy similar. 
  Para ello buscamos el valor de λ para el cual obtenemos el primer conjunto con 7 variables.
  
 
```{r MEJORES 11 VARIABLES  } 
  
first_ELEVEN <- max(which(cv_error_lasso$nzero == 11))
my_lam <- cv_error_lasso$lambda[first_ELEVEN]
out_ELEVEN <- glmnet(x,y,alpha=1,lambda = my_lam)
lasso_coef_ELEVEN <- predict(out_ELEVEN,type="coefficients")
lasso_coef_ELEVEN


```   




```{r comparing Ridge and Lasso  }

par(mfrow = c(1,2))
plot(cv_error_ridge,ylab = "Mean Square Error ridge regression" )
abline(h = 120000)
plot(cv_error_lasso,ylab = "Mean Square Error lasso")
abline(h = 120000)

par(mfrow = c(1,1))
```   



# GAM

 
## How Regression Splines Work

  Cubic B-spline requiere 2(q+1)+m .
  Los inners Knots son lo puntos de estimación necesarios para calcular una smooth función, es decir como una guía de la función smooth¿?, y de qué depende¿? de la variable x¿? The boundary knots son arbitrarios ( esto es aleatorio) y se sitúan en los extremos de los inner knots.¿?
  Entiendo que son los nudo por donde pasa la Spline a mayor número Overfitting a menor mala estimación.
  
  RSS= Suma (yi-g(xi))2 ---> Overfit data , makes RSS small but that also smooth
  How g is smooth, function
  
  Sum (yi-g(xi))2 +lambda *integral ( g''(t)2 dt)
  donde Lambda is not-negative 
  
  
## Configuración espacial continua




```{r GAM model  Lasso LAMBDA optimal  }

## gam regression
## listado de variables seleccionadas a partior del Lasso

model_gam <- gam (price~ s(lat,long) +s(sqft_living)  +s(sqft_living15) + s(sqft_lot15) + s(bedrooms)  + s(bathrooms)  + s(grade)   + waterfront + s(condition,k=3) + s(view,k=3) + s(yr_built) , data=train, method="REML")


summary(model_gam)
layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam,view=c("lat","long"),plot.type="persp")
plot(model_gam, scale = 0,shade = TRUE, shade.col = "lightblue")


```   
### Añadimos la constante para la interpretabilidad de los Smooths

```{r Añadimos la constante para la interpretabilidad de los Smooths}

layout(matrix(1:2, ncol = 2))
plot(model_gam,shade = TRUE, shade.col = "lightblue", shift=coef(model_gam)[1])

```

 Viendo los gráficos podemos afirmar que La variable Condition es Lineal y View es posible que también,
 ambas variables son siginificativas segúne el p-valor y el valor de " edf" en condition es muy cercano a 1 por lo que podemos afirmar que ###  se ajusta a una relación lineal.
 La variable view es muy cercana a 2 y es significativa. No es lineal

### Relanzamos el modelo GAM  incluyendo la variable condition sin Smooth, Versión 2

```{r GAM model  Lasso LAMBDA optimal 2}

## gam regression
## listado de variables . Condition como Lineal


model_gam2 <- gam (price~ s(lat,long) +s(sqft_living)  +s(sqft_living15) + s(sqft_lot15) + s(bedrooms)  + s(bathrooms)  + s(grade)   + waterfront + condition + s(view,k=3) + s(yr_built) , data=train, method="REML")


summary(model_gam2)
layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam2,view=c("lat","long"),plot.type="persp")
plot(model_gam, scale = 0,shade = TRUE, shade.col = "red" , shift=coef(model_gam2)[1])


```


## checking concurvity

La concurrencia ocurre cuando algún término suave en un modelo puede ser aproximado por uno o más de los otros términos suaves en el modelo.
Este es a menudo el caso cuando se incluye una suavidad de espacio en un modelo, junto con suavidades de otros
covariables que también varían más o menos suavemente en el espacio. Del mismo modo tiende ser un problema en modelos que incluyen un tiempo suave, junto con problemas de otro tiempo variando covariables.

Es esencialmente la forma no lineal de colinealidad, la library mgcv proporciona una medida de concurrencia y establece en la documentación si tales medidas están por encima de> 1.0 es preocupante.
La literatura nos dice que se puede esperar concurvity con datos espaciales como es nuestro caso.



La concurrencia puede verse como una generalización de la co-linealidad y sus causas.

índices Relacionados Concurvity, todos entre [0,1]:
1.-Worst
2.-Observed
3.-Estimate



```{r concurvity}

# Check pairwise concurvity
k<-concurvity(model_gam2,full=FALSE)

#options(scipen=999)
k$worst

```



### Relanzamos el modelo GAM  AFTER CONCURVITY  Versión 3
### Excluímos del Modelo la variable bathrooms por alta Concurvity 0.8319


```{r MODEL AFTER CONCURVITY}

# eliminamos la variable bathrooms
#   s(sqft_living)*s(bathrooms)  0.8319

model_gam3 <- gam (price~ s(lat,long) +s(sqft_living)  +s(sqft_living15) + s(sqft_lot15) + s(bedrooms)  + s(grade)   + waterfront + condition + s(view,k=3) + s(yr_built) , data=train, method="REML")


summary(model_gam3)
layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam3,view=c("lat","long"),plot.type="persp")
plot(model_gam3, scale = 0,shade = TRUE, shade.col = "green", shift=coef(model_gam3)[1])

#concurvity(model_gam,full=FALSE)

```



### GAM CHECK Resids vs linear pred


```{r test data GAM Price}

par(mfrow = c(1, 2))
plot(model_gam3, residuals = TRUE, pch = 1)
gam.check(model_gam3)
```



Podemos ver ahora (gráfico de Resids vs linear pred) que se incumple la hipótesis de varianza constante ya que la variabilidad de los residuos aumenta cuando lo hace el predictor lineal. Este tipo de problemas se suele arraglar con la tranasformación raíz cuadrada. Aplicamos dicha transformación y ajustamos el nuevo modelo


### Relanzamos el modelo GAM  TRASFORMACION CUADRÁTICA  Versión 4

```{r tranformacion cuadratica}

# Modelo

model_gam4 <- gam (sqrt(price)~ s(lat,long,k=29) +s(sqft_living)  +s(sqft_living15) + s(sqft_lot15,k=5) + s(bedrooms)  + s(grade,k=8)   + waterfront + condition + s(view,k=3) + s(yr_built) , data=train, method="REML")

#model_gam2 <- gam (sqrt(price)~ s(lat,long,k=20) +s(sqft_living)  +s(sqft_living15) + s(sqft_lot15,k=5) + s(bedrooms)  + s(grade,k=8)   + waterfront + condition + s(view,k=3) + s(yr_built) , data=train, method="REML")

summary(model_gam4)

layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam4,view=c("lat","long"),plot.type="persp")
plot(model_gam4, scale = 0,shade = TRUE, shade.col = "orange", shift=coef(model_gam4)[1])

```

## GAM FIT

Verificar un GAM fitted es como verificar un GLM fitted pero con dos diferencias fundamentales:
  1.-Las dimensiones básicas utilizadas para los términos suaves deben verificarse 
  2.-En segundo lugar, supuestos de distribución GLM regular, los GAM pueden ser más sensibles. Por ello se usa un gráfico QQ residual mejorado.
  
  
Esta función representa 4 gráficos de diagnóstico estándar

  1.-The test of whether the basis dimension for a smooth is adequate. Ver si el nivel de Smooth es el adecuado.
  Los valores p bajos pueden indicar que la dimensión base, k, se ha establecido demasiado baja, especialmente si el EDF informado está cerca de k ', el FED máximo posible para el término. Duplicar a un 'k' sospechoso y volver a ajustarlo es una buena prueba
  2.-QQ Gam 
  3.-Distribución de la varianza de los errores
  4.-Response vs Fitted Value.

  
```{r Bondad del Ajuste}
# Diagnóstico
gam.check(model_gam4)
```


#### Checking del Smooth

Vemos que sería adecuado modificar los valores K en las siguientes Variabes.s(lat,long),s(sqft_lot15,s(bedroom)
Mostramos el último resultado seleccionado


### Relanzamos el modelo GAM  Modificando los K  Versión 5

```{r Bondad del Ajuste2}
# Diagnóstico

model_gam5 <- gam (sqrt(price)~ s(lat,long,k=20) +s(sqft_living,k=8)  +s(sqft_living15) + s(sqft_lot15,k=3) + s(bedrooms)  + s(grade,k=6)   + waterfront + condition + s(view,k=3) + s(yr_built,k=8) , data=train, method="REML")

summary(model_gam5)

# Diagnóstico
gam.check(model_gam5)

```


### Comparamos RME  modelo GAM1  modelo GAM5

```{r TESTING MODEL GAM VERSION 5}

pred.modelo.gam1 <-  predict(model_gam, control)
new1=sqrt(control$price)
test.error.gam1 <- mean((pred.modelo.gam1 - new1)^2)

pred.modelo.gam2 <-  predict(model_gam2, control)
new2=sqrt(control$price)
test.error.gam2 <- mean((pred.modelo.gam2 - new2)^2)

pred.modelo.gam3 <-  predict(model_gam3, control)
new3=sqrt(control$price)
test.error.gam3 <- mean((pred.modelo.gam3 - new3)^2)

pred.modelo.gam4 <-  predict(model_gam4, control)
new4=sqrt(control$price)
test.error.gam4 <- mean((pred.modelo.gam4 - new4)^2)


pred.modelo.gam <-  predict(model_gam5, control)
new5=sqrt(control$price)
test.error.gam5 <- mean((pred.modelo.gam - new5)^2)
test.error.gam5

```

### Representación Gráfica de estos RME
```{r}
modelo <- c("GAM1", "GAM2",  "GAM3",  "GAM4", "GAM5")
test.MSE <- c(test.error.gam1,test.error.gam2,test.error.gam3,test.error.gam4, test.error.gam5)

comparacion <- data.frame(modelo = modelo, test.MSE = test.MSE)

ggplot(data = comparacion, aes(x = reorder(x = modelo, X = test.MSE), 
                               y = test.MSE, color = modelo, 
                               label = round(test.MSE,2))) + 
geom_point(size = 8) + 
geom_text(color = "white", size = 2) + 
labs(x = "Grupo de GAM's", y = "Test error(MSE)") + theme_bw() + 
coord_flip() + theme(legend.position = "none")
```

### cáculo del R2 Modelo 4 para la poblacion Train
```{r ajusted R2 model4 Train}
summary(model_gam4)$r.sq 
```


### cáculo del R2 Modelo 4 para la poblacion Control
```{r ajusted R2 model4 test}
SST <- mean((new4 - mean(new4))^2)
SSE <- test.error.gam4
R2 <- 1 - SSE / SST
R2
```

### cáculo del R2 Modelo 5 para la poblacion Train
```{r ajusted R2 model5 Train}
summary(model_gam5)$r.sq 
```


### cáculo del R2 Modelo 5 para la poblacion Control
```{r ajusted R2 model5 test}
SST <- mean((new5 - mean(new5))^2)
SSE <- test.error.gam5
R2 <- 1 - SSE / SST
R2
```



```{r ANOVA de los Modelos }
anova(model_gam, model_gam2,model_gam2,model_gam3,model_gam4,model_gam5, test="Chisq")
AIC(model_gam,model_gam2,model_gam3,model_gam4,model_gam5)

```




