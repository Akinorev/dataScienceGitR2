---
title: "Cluster analysis"
author: "Master Data Science"
date: "9/2/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)

library(cluster)
library(dplyr)
library(ggplot2)
library(ggmap)
library(readr)
library(factoextra)
library(Rtsne)
library(tidyverse)
library(plotly)


#para poder usa TSNE es necesario
#install.packages("Rtsne")
#if(!require(devtools)) install.packages("devtools") devtools::install_github("jkrijthe/Rtsne")

```

## Técnicas No Supervisadas. Analisis Cluster

Necesitamos analizar los datos de tipo mixto, número, órdinal y nominal.
Nos vamos a enfocar en clasificación no supervisada usando R
CLUSTERING ALGORITHM: PARTITIONING AROUND MEDOIDS (PAM)

```{r pwd ruta fichero}

#setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/PRAC/Cluster")
df_mas <- read.csv ("kc_house_data.csv")
#set.seeds=737
#df_mas  <- sample_n(df_mas, size = 10000)
```

## Distancia de Gower
La distancia es una medida numérica de cuán separados están los individuos, es decir una métrica utilizada para medir la proximidad o similitud entre individuos;

La distancia de Gower se calcula como el promedio de las diferencias parciales entre individuos.
Para cada tipo de variable, se usa una métrica de distancia particular que funciona bien para ese tipo y se escala para caer entre 0 y 1

Para las variables cuantitativa La Distancia de Manhattan
Para las Variables Ordinales la Distancia un ajuste especial de la Manhattan despúes de haber sido ordenadas
Para las Nominales primero se convierte en k columnas Binarias ( para cada categoria de cada variable norminal) y posteriormente se usa el coeficiente de Dice

El coeficiente de Dice [0,1] para medir la similitud entre 2 muestras

Se escala de la siguiente Manera
Se define la distancia de Gower como d2ij = 1 − sij , 
donde sij = p1h=1 (1 − |xih − xjh|/Gh) + a + α p1 + (p2 − d) + p3 es el coeficiente de similaridad de Gower,

p1 es el numero de variables cuantitativas continuas,
p2 es el numero de variables binarias,
p3 es el numero de variables cualitativas(no binarias),
a es el numero de coincidencias (1, 1) en las variables binarias,
d es el numero de coincidencias (0, 0) en las variables binarias,
α es el numero de coincidencias en las variables cualitativas (no binarias) y
Gh es el rango (o recorrido) de la h-esima variable cuantitativa.




```{r distancia de Gower}

gower_dist <- daisy(df_mas, metric = "gower")
gower_mat  <- as.matrix(gower_dist)

```

## Ejemplo de Casas Más similares, basado en la distancia de Gower

```{r mas similares}
df_mas[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), 
              arr.ind = TRUE)[1, ], ]

```

## Ejemplo de Casas Más Disimilares, basado en la distancia de Gower

```{r mas disimilares}
df_mas[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), 
              arr.ind = TRUE)[1, ], ]

```



## Selección del algoritmo PAM

Una vez calculada la matriz de distancia emplearemos el algoritmo PAM, basado en una partición de medoids (El término medoids se refiere a un objeto dentro de un grupo para el cual la diferencia promedio entre este y todos los demás miembros del grupo es mínima, es decir el punto más centralmente ubicado del conjunto de datos), en cambio en el método K-means cada Cluster está representado por su centroide.
Es un método muy similar a k-means, pero es mucho más robusto a la presencia de Outliers como es en nuestro caso.
Es un procedimiento de agrupación iterativa que implica los siguientes pasos:


## Step1

  Elejir k entidades aleatorias para convertirse en los Medoids
  
## Step2

  Asignamos a cada entidad, en nuestro caso a cada "casa" el medoide más cercano basado en la distancia de Gower anteriormente calculada.
  
## Step3

  Para cada Cluster identificar la observación que produciría la distancia promedio más baja si fuera reasiganada como el Medoid, si fuera así hay que hacer de esta observación el nuevo Medoid. Si al menos un Medoid ha cambiado volvemos Step2, en caso contrario Step4
  
## Step4

  FIN

K Means intenta mininizar el ECM total K Medoids minimiza la suma de las diferencias entre los puntos etiquetados para estar en un grupo y un punto designado como el centro de ese grupo Mediod.


## Selección del Número óptimo de Clústers

Silhouette, Validación y consistencia dentro de los datos. 

Es una medida de cuan similar es objeto dentro del grupo de pertenencia y cuan disimilar con los otros grupos.

Varía entre -1 y 1. Un valor alto indica que un objeto está bien emparejado dentro de su grupo y mal con el resto.
Un valor muy bajo o negativo implica una revisión del número de cluster al alza o a la baja



```{r Clúster number selection }


sil_width <- c(NA)
for(i in 2:10){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
plot(1:10, sil_width,
     xlab = "Numero de clusters",
     ylab = "Silhouette")
lines(1:10, sil_width)
#fviz_silhouette(pam_fit)


```

Después de calcular el Silhouette para el algoritmo PAM vemos que 2 grupos producen el valor más alto. 
Aún asi nosotros seleccionamos 3 Cluster para dividir la dispersión del Trabajo y facilitar el entendimiento de los siguientes análisis.

## Seleccionamos 3 Clusters 

```{r Clúster interpretacion }


set.seed=737
k <- 3
pam_fit <- pam(gower_dist, diss = TRUE, k)
pam_results <- df_mas %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
#frecuencia del número de casas en cada Clúster
ftable(pam_fit$clustering)


```

## Representación Mapas 



```{r Clúster maps }


df_mas$cluster<-pam_fit$clustering

maps <- data.frame (long=df_mas$long, lat=df_mas$lat, cluster=df_mas$cluster, precio=df_mas$price,code=df_mas$zipcode)

states <- map_data("state")
dim(states)
washington <- subset(states, region %in% c("washington"))

ca_base <- ggplot(data = washington, mapping = aes(x = long, y = lat, group = group, fill=group)) + coord_fixed(1.3) +  geom_polygon(color = "black", fill = "white")
counties <- map_data("county")
ca_county <- subset(counties, region == "washington")

ca_base  + 
  geom_polygon(data = ca_county, fill = NA, color = "white") +
  geom_polygon(color = "black", fill = NA) + scale_fill_identity() +
  ggtitle("washington") +
  geom_point(data = maps, aes(x = long,y = lat), color=maps$cluster, size = 0.1 ,inherit.aes = FALSE )
  

ca_base  + 
  geom_polygon(data = ca_county, fill = NA, color = "white") +  coord_map(xlim = c(-123,-121),ylim = c(47, 48))+ 
  geom_polygon(color = "black", fill = NA) + 
  ggtitle("washington") + guides(fill=FALSE) +
  geom_point(data = maps, aes(x = long,y = lat), color=maps$cluster, size = 0.5  ,inherit.aes = FALSE )
  



df_mas %>%
        group_by(cluster) %>%
        summarise(num_casas=n(), precio= mean(price), room=mean(bedrooms),baths=mean(bathrooms) ,
                  tamanyo=mean(sqft_living), anyo=mean(yr_built), grade=mean(grade), floor=mean(floors) ,visitas=mean(view))

```




## Cluster 1 Rojo. Casas Medianas

</br>
Grupo de Precio Mediano, con menor número de Habtaciones y Baños en Media.
Menor tamaño de metros cuadrados de la vivienda.
Grupo de Peor Grado de Vivienda.
Casas de mayor Antigüedad 
</br>


## Cluster 2 Negro. Casas de Precio Bajo

</br>
Grupo de Precio Más Bajo.
Grupo que menor número de visitas Recibe, casí todas  de planta única.
Casas de los años 60,70.
</br>


## Cluster 3 Verde. Casas Precio Alto

</br>
Precio Más Alto en Media , mayor número de Baños, metros, habitaciones y Plantas
Casas más nuevas
</br>



## Reducción de dimensionalidad para visualizar los Clústers.

## Visualizacion  PCA (linear)

El PCA es un algoritmo lienal, no podrá interpretar relaciones complejas polinómicas entre los items del dataset.
Vemos que en los dos primeros componentes recogemos el 50,35% de la variabilidad
Seleccionando aquellos componentes con autovalor mayor a 1,los Cuatro Primeros, explican el 71,5% de la variabilidad total.


```{r pca dimensiones }

data<- subset(df_mas, select=c("price", "bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront",                                        "view","condition","grade","sqft_above","sqft_basement","sqft_living15","sqft_lot15"))
pca<- prcomp(data, scale=TRUE)

summary (pca)

```

## ScreePlot refleja el porcentanje de variabilidad explicada por cada Componente Principal

```{r pca screeplot }
fviz_eig(pca)

```

## Interpretación de cada uno de los componentes Principales

A través de los Scores de cada variable dentro del componente Principal podemos libremente afirmar y con fines explicativos;
 
##PC1

  Precio/m2       38% de la variabilidad
  
##PC2

  Espacio Sotano  12% de la variabilidad
  
##PC3

  Espacio Parcela 12% de la varibilidad
  
##PC4

  Vistas al Mar    9% de la variabilidad

```{r pca explicación }
pca

```
## Cluster dentro de los PCA's 

Visualizaremos nuestros 3 Clústeres dentro del plano PC1 y PC2, 
Observamos que tenemos problemas los valores extremos por un lado y por otro la gran partes de los items se nos acumulan muy próximos lo cual no nos facilita la comprensión de los Grupos.

Planteamos nuesvas alternativas al entender que el PCA es insuficiente para interpretar nuestros Clusters


```{r PCA plano con los Clusteres }

#join pca data y df cluster


pam_fit$clustering <- as.character(pam_fit$clustering)
pca_data <- data.frame(pca$x, cluster=pam_fit$clustering)

ggplot(pca_data, aes(x=PC1, y=PC2, color=cluster)) + geom_point()



```





## Visualizacion  t-SNE (non-parametric/ nonlinear)
## https://lvdmaaten.github.io/tsne/
## https://cran.r-project.org/web/packages/tsne/tsne.pdf

Algoritmo de reducción de dimensionalidad no lineal, encuentra patrones en los datos mediante la identificación de grupos observados basados en la similitud de puntos de datos con múltiples características.

Esta técnica permite utilizar la métrica anteiormente creada, la Distancia de Gower, en nuestro caso se muestran los tres grupos que seleccionamos anteriorenteme con el algortimo PAM.

Asigna los datos multidimensionales creados en la Distancia de Gower anteriormente calculada a un espacio dimensional menor.

## Algoritmo  t-SNE

  Muy útil para el "crowding problem" que implica "la maldición de la dimensión" y básicamente en nuestro caso afecta ya que al aumentar el número de dimensiones la distancia al vecino más próximo aumenta.

## Step1
  Comienza convirtiendo la distancia, nuestro caso Gower, entre los puntos de datos en medidas de probabilidad condicionales que representan similtud entre los datos.
  Función Gaussiana, probabilidad alta y probabilidad Baja
  Hay que prestar atención a las colas que son estrechas y pueden acumular mucha relación de puntos.
  

## Step2 
  Representando la distribución de Probabilidad.
  La idea intuitiva es realizar asignaciones de baja dimensión que representen distribuciones de probabilidad similares, aquí nos podemos encontrar con "crowding problem" debido a las "colas cortas" de las distribuciones Gaussianas. Para subsanar este problema y que los puntos tengan una "cola más larga" la t-sne usa uns distribución T-stundent con un grado de libertad.
  La optimización de esta distribución t-student se realiza mediante una función Gradiente Descendiente que intuitivamenete representa la fuerza y la atracción-repulsión entre dos puntos. Gradiente positivo implica atracción y al contrario.
  Este "push-and-pull" hace que los puntos se asienten en espacio de baja dimensionalidad.
  
## Step 3
  Las t-snes no tienen parámetros y optimizan directamente a través de la función de Coste Gradiente que es No Convexa y puede darnos problemas con los mínimos locales.
  Existen funciones para corregir este crecimiento de la función Gradiente sobre todo al comienzo del algoritmo.
  Importante el concepto de vecino estocásticos lo cual implica que no está cerrada la frontera de los puntos que son  vecinos contra los puntos que no lo son permitiendo al algotimo tener en cuenta la estructura local como la global.( esto lo realizaremos con el parámetro perplexity)
  
  
En la siguiente visualización aparecen los 3 clúster dentro los ejes X e Y
El número de dimensiones por defecto es 2


Perplexity es el parámetro que usamos para equilibrar el aspecto local y global de los datos, es en cierta medida determinar de forma supuesta cuanto es el número de vecinos quye tendría cada item ( en nuestro ejemplo casa) por defecto es 20, si lo concentramos mucho o lo dispersamos mucho 





## perplexity=50

```{r Clúster p50 }
set.seed=737
tsne_obj <- Rtsne(gower_dist,perplexity = 50, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))

#join tsne data y df original

df_join<-cbind(tsne_data,df_mas) 


plot_ly(
  df_join, x = ~X, y = ~Y,
  color = ~yr_built, size = ~sqft_living)

plot_ly(
  df_join, x = ~X, y = ~Y,
  color = ~price, size = ~sqft_living)





```




## perplexity=80

```{r Clúster p80 }
set.seed(767)
tsne_obj <- Rtsne(gower_dist,perplexity = 80, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))
set.seed(767)
ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))

#join tsne data y df original
set.seed(767)
df_join<-cbind(tsne_data,df_mas) 

set.seed(767)
plot_ly(
  df_join, x = ~X, y = ~Y,
  color = ~yr_built, size = ~sqft_living)


```


## New CLUSTER After TSNE
  
  El objeto tsne_obj$Y contiene las coordenadas X-Y para cada caso de entrada.

## Hierarchical clustering

  Agrupaciones de Clusters anidados de forma Aglomerativa, cada observación comienza en su propio grupo, y los pares de grupos se van fusionando a medida que uno se mueve hacia arriba en la jerarquía.
  El link Criteria que emplearemos será el de Ward que minimizala suma de diferencias entre los Clusters.
  Probamos con 7 clúster a partir del dendograma y analizamos los resultados.
  


```{r Cluster Jerarquico  }
set.seed(767)
cluster_hierarchical=hclust(dist(tsne_obj$Y), method = "ward.D")
plot(cluster_hierarchical, cex = 0.6, hang = -1)

set.seed(767)
df_join$hclust = factor(cutree(cluster_hierarchical, 9))

set.seed(767)
prueba <- subset( df_join, select = -cluster )
ggplot(aes(x = X, y = Y), data = prueba) +
  geom_point(aes(color = hclust))


write.csv(prueba,file="cluster.csv")


```

