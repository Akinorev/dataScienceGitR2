---
title: "GAM Binary"
author: "Master Data Science: Verónica Gómez, Carlos Grande, Pablo Olmos"
date: "2020"
output:
  html_document:
    theme: united
    code_folding: "hide"
    toc: yes
    toc_float: yes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(dplyr)
library(ggplot2)
library(readr)
library(factoextra)
library(Rtsne)
library(caret)
library(lattice)
library(glmnet)
library(pscl)
library(ROCR)
library(VarReg)
library(mgcv)
library(brew)
library(bsplus)
library(DMwR2)
library(car)
library(carData)
library(caret)
library(cluster)
library(dplyr)
library(egg)
library(expss)
library(factoextra)
library(faraway)
library(gclus)
library(GGally)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(Hmisc)
library(htmltools)
library(ISLR)
library(kableExtra)
library(knitr)
library(magrittr)
library(mice)
library(mlbench)
library(RColorBrewer)
library(readr)
library(Rtsne)
library(sos)
library(tidyr)
library(tidyverse)
library(VIM)
library(nortest)
library(scales)
library(plyr)
library(PerformanceAnalytics)
library(corrplot)
library(leaps)
library(plotly)
library(mboost)
library(nnet)
library(stargazer)
library(ROCR)

library(ROCit)
```

# 2. Modelo GAM. Variable Dependiente Binaria.

Vamos a trabajar el modelo GAM  con una variable Binaria 


### Cargamos los Data Sets

```{r cargamos los data sets }


setwd("C:/Users/Pablo/Desktop/Machine_Learning_I/Z_PRACTICA_MACHINE_LEARNING/machineLearning1Process")
train      <- read.csv ("train.csv")
validation <- read.csv ("validation.csv")
test <- read.csv ("test.csv")

```




# Ridge regression

  Intenta minimizar el RSS, ridge regression incorpora un término llamado shrinkage penalty que fuerza a que los coeficientes de los predictores tiendan a cero controlada por el parámetro λ. 
  Cuando λ=0 la penalización es nula y los resultados son equivalentes a los obtenidos por mínimos cuadrados, cuando λ=∞ todos los coeficientes son cero.
  La principal ventaja es la reducción de Varianza.
  Si  todos los predictores incluidos tienen coeficientes diferentes a cero (todos contribuyen al modelo) y aproximadamente de la misma magnitud, ridge regression tiende a funcionar mejor.
  
  Para realizar ridge regression se va a emplear la función glmnet() del paquete glmnet.
 
 
```{r tunning parameter LAMDA }

#matriz con las valores de los predictores para cada observación y un vector y=target variable respuesta
x <- model.matrix(target~  (bedrooms+ bathrooms+floors+sqft_living+grade+condition+view+waterfront+                           sqft_lot+sqft_above+sqft_basement+yr_built+yr_renovated+yr_renovated+
                            sqft_living15+sqft_lot15+Cluster_final), data = train)[, -1]
head(x)
y <- train$target
y <- as.integer(y)



# Para obtener un ajuste mediante ridge regression se indica argumento alpha=0.
modelos_ridge <- glmnet(x = x, y = y, alpha = 0)
plot(modelos_ridge, xvar = "lambda", label = TRUE)

```     
  
  Al aumentar el tamaño de los Lambda dismunuyen los coeficientes.
  Con el fin de identificar el valor de λ que da lugar al mejor modelo, se puede recurrir a Cross-Validation. La función cv.glmnet() calcula el cv-test-error, utilizando por defecto k=10.
  
 
```{r select  parameter LAMBDA in Ridge Regression}  
  
set.seed(737)
cv_error_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 15,
                            type.measure = "mse")
plot(cv_error_ridge)
```
  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -2, pero el menor error cuadrático medio se da para 17 variables regresoras y se mantiene constante. Es una de las grandes diferencias con Lasso.




```{r select  min parameter LAMBDA in Ridge Regression}  
  
# Valor lambda con el que se consigue el mínimo test-error
cv_error_ridge$lambda.min

```


```{r select  optimal  parameter LAMBDA in Ridge Regression}  
# Valor lambda óptimo: mayor valor de lambda con el que el test-error no se
# aleja más de 1 sd del mínimo test-error posible.
cv_error_ridge$lambda.1se
```

  
  
```{r modelo final Ridge}  
# Se muestra el valor de los coeficientes para el valor de lambda óptimo

modelo_final_ridge <- glmnet(x = x, y = y, alpha = 0, lambda = cv_error_ridge$lambda.1se)
coef(modelo_final_ridge)

```



  
# Lasso

  El método lasso, al igual que ridge regression, fuerza a que las estimaciones de los coeficientes de los predictores tiendan a cero. La diferencia es que lasso sí es capaz de fijar algunos de ellos exactamente a cero, lo que permite además de reducir la varianza, realizar selección de predictores.
  ∑i=1n(yi−β0−∑j=1pβjxij)2+λ∑j=1p|βj|=RSS+λ∑j=1p|βj|
  
  Cuando solo un pequeño número de predictores de entre todos los incluidos tienen coeficientes sustanciales y el resto tienen valores muy pequeños o iguales a cero, lasso genera mejores modelos.
  
  
Selección del tunning parameter λ
  
    Determinar el grado de penalización, seleccionamos un rango de valores de λ y se estima el cross-validation error resultante para cada uno, finalmente se selecciona el valor de λ para el que el error es menor y se ajusta de nuevo el modelo, esta vez empleando todas las observaciones.
    

```{r tunning parameter  }

modelos_lasso <- glmnet(x = x, y = y, alpha = 1)
plot(modelos_lasso, xvar = "lambda", label = TRUE)

set.seed(737)
cv_error_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 10)
plot(cv_error_lasso)

```     

  Podemos observar cómo varía el error cuadrático medio, en función del valor de regularización. 
  Gráficamente se comprueba que el error no aumenta hasta que las variables con coeficiente mayor que cero es menor que -4, pero el menor error cuadrático medio se da para 3 variables regresoras. 


```{r tunning parameter Lasso LAMBDA }

cv_error_lasso$lambda.min
cv_error_lasso$lambda.1se

# Se reajusta el modelo con todas las observaciones empleando el valor de
# lambda óptimo

modelo_final_lasso <- glmnet(x = x, y = y, alpha = 1, lambda = cv_error_lasso$lambda.1se)
coef(modelo_final_lasso)
    
```    
  
  la ventaja del modelo final obtenido por lasso es que es mucho más simple ya que contiene únicamente 'n' predictores
  A continuación, ajustamos un modelo de regresión con el λ para las variables significativas 
  No obstante, como se observa en la gráfica del error podríamos obtener un modelo con sólo 10 variables cuyo error es muy similar. 
  Para ello buscamos el valor de λ para el cual obtenemos el primer conjunto con 10 variables.
  
 

```{r comparing Ridge and Lasso  }

par(mfrow = c(1,2))
plot(cv_error_ridge,ylab = "Mean Square Error ridge regression" )
abline(h = 120000)
plot(cv_error_lasso,ylab = "Mean Square Error lasso")
abline(h = 120000)

par(mfrow = c(1,1))
```   


# GAM link Logistic
### Modelo1 PostRegularización.

```{r model GAM Logit1}


# Modelo

model_gam_log1 <- gam (target~ s(lat,long) +s(sqft_living) + s(grade) + s( sqft_above)  + s(yr_built) + s(sqft_living15) +s(floors,k=3)+ s(view,k=3) + s(condition,k=3) + Cluster_final,
                       data=train, family =  binomial("logit"), method="REML")

summary(model_gam_log1)

layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam_log1,view=c("lat","long"),plot.type="persp")
plot(model_gam_log1, scale = 0,shade = TRUE, shade.col = "lightblue", shift=coef(model_gam_log1)[1])
      

```

Analizando la salida vamos tomar las siguiente decisiones:

##### 1.-Excluímos la varaible Clúster del Análisis ya que de No se Rechaza la Ho de parámetro Lineal igual a cero para la clase Media, vamos a situarla de Referencia de Cluster a "med"
##### 2.-Vemos que tenemos variables que son significativas pero que son lineales por lo que quitaremos la función smooth de ellas, y estudiaremos si son lienalmente significativas. Son: Condition y Floors. Quizá sqft_living15 también pudiera ser lineal pero de momento la vamos a mantener tal cual 


### Modelo2 PostRegularización. Check Variables.

```{r model GAM Logit2}

# Modelo
train$Cluster_final <- relevel(train$Cluster_final, ref = "med")
model_gam_log2 <- gam (target~ s(lat,long) +s(sqft_living) + s(grade) + s( sqft_above)  + s(yr_built) + s(sqft_living15) +floors+ s(view,k=3) + condition + Cluster_final,
                       data=train, family =  binomial("logit"), method="REML")

  

summary(model_gam_log2)
layout(matrix(1:2, ncol = 2))
vis.gam (x=model_gam_log2,view=c("lat","long"),plot.type="persp")
plot(model_gam_log2, scale = 0,shade = TRUE, shade.col = "green" , shift=coef(model_gam_log2)[1])
    
```


Analizando la salida vamos tomar las siguiente decisiones:

##### 1.-No ha habido un gran cambio debido al cambio en la referencia del Clúster.
##### 2.-Vamos a considerar la variable sqft_living15 como no lineal, Aunque el edf es próximo a uno y podríamos plantearnos considerarla a lineal. El resto de variables son todas significativa (salvo una una categoria del Cluster)
##### 3.-El valor del R2 no es especialmente bueno 0.723




## checking concurvity Model 2 PostRegularización

La concurrencia ocurre cuando algún término suave en un modelo puede ser aproximado por uno o más de los otros términos suaves en el modelo.
Este es a menudo el caso cuando se incluye una suavidad de espacio en un modelo, junto con suavidades de otros
covariables que también varían más o menos suavemente en el espacio. Del mismo modo tiende ser un problema en modelos que incluyen un tiempo suave, junto con problemas de otro tiempo variando covariables.

Es esencialmente la forma no lineal de colinealidad, la library mgcv proporciona una medida de concurrencia y establece en la documentación si tales medidas están por encima de> 1.0 es preocupante.
La literatura nos dice que se puede esperar concurvity con datos espaciales como es nuestro caso.



La concurrencia puede verse como una generalización de la co-linealidad y sus causas.

índices Relacionados Concurvity, todos entre [0,1]:
1.-Worst
2.-Observed
3.-Estimate



```{r concurvity}

# Check pairwise concurvity
k<-concurvity(model_gam_log2,full=FALSE)

#options(scipen=999)
k$worst

```

No existen variables con Concurvity significatica.


## Matriz de Confusión GAM binary para la población Validation


```{r Matrix Confusion Validate  }

predictions_val <- predict(model_gam_log2, validation,type='response')

## Warning: package 'ROCit' was built under R version 3.5.2
ROCit_obj <- rocit(score=predictions_val,class=validation$target)
plot(ROCit_obj)

table(pred = predictions_val > 0.5, obs = validation$target)
data = as.numeric(predictions_val>0.5)
data=as.factor(data)
y_test=as.factor(validation$target)

# use caret and compute a confusion matrix
confusionMatrix(data, y_test)
        
      
```

## GAINS and Lift Chart


```{r Gains Char and Lift}

data = as.numeric(predictions_val>0.5)
gtable15 <- gainstable(score = data, 
                       class = validation$target,
                       
                       ngroup = 15)


plot(gtable15, type = 3,main='')
title(main = "Gains Chart Validation Population")

plot(gtable15, type = 1)
title(main = "Lift Chart Validation Population")

```


  
  
